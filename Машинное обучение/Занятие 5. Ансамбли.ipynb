{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайный лес\n",
    "\n",
    "Лео Брейман нашел применение бутстрэпу не только в статистике, но и в машинном обучении. Он вместе с Адель Катлер усовершенстовал алгоритм случайного леса, который был предложенный [Хо](http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf), добавив к первоначальному варианту построение некоррелируемых деревьев на основе CART, в сочетании с методом случайных подпространств и бэггинга. \n",
    "\n",
    "Решающие деревья являются хорошим семейством базовых классификаторов для бэггинга, поскольку они достаточно сложны и могут достигать нулевой ошибки на любой выборке. Метод случайных подпространств позволяет снизить коррелированность между деревьями и избежать переобучения. Базовые алгоритмы обучаются на различных подмножествах признакового пространства, которые также выбираются случайным образом.\n",
    "Ансамбль моделей, использующих метод случайного подпространства, можно построить, используя следующий алгоритм:\n",
    "\n",
    "1. Пусть количество объектов на тренировочной выборке равно $\\large N$, а количество признаков $\\large D$.\n",
    "2. Выбираем $\\large L$ - число отдельных моделей в ансамбле.\n",
    "3. Для каждой отдельной модели $\\large l$ берем $\\large dl (dl < D) $ как число признаков для $\\large l$ . Обычно для всех моделей используется только одно значение $\\large dl$.\n",
    "4. Для каждой отдельной модели $\\large l$ создаем обучающую выборку, выбрав $\\large dl$-признаков из $\\large D$ с замещением и обучаем модель.\n",
    "\n",
    "Теперь, чтобы применить модель ансамбля к новому объекту, объединяем результаты отдельных $\\large L$ моделей мажоритарным голосованием или путем комбинирования апостериорных вероятностей.\n",
    "\n",
    "\n",
    "## Алгоритм\n",
    "\n",
    "Алгоритм построения случайного леса, состоящего из $\\large N$ деревьев, выглядит следующим образом:\n",
    "* Для каждого $\\large n = 1, \\dots, N$:\n",
    "     * Сгенерировать выборку $\\large X_n$ с помощью bootstrap.\n",
    "     * Построить решающее дерево $\\large b_n$ по выборке $\\large X_n$:\n",
    "         — по заданному критерию мы выбираем лучший признак, делаем разбиение в дереве по нему и так до исчерпания выборки\n",
    "         — дерево строится, пока в каждом листе не более $\\large n_\\text{min}$ объектов или пока не достигнем определенной глубины дерева\n",
    "         — при каждом разбиении сначала выбирается $\\large m$ случайных признаков из $\\large n$ исходных, \n",
    "         и оптимальное разделение выборки ищется только среди них.\n",
    "         \n",
    "Итоговый классификатор $\\large a(x) = \\frac{1}{N}\\sum_{i = 1}^N b_i(x)$, простыми словами — для задачи кассификации мы выбираем решение голосованием по большинству, а в задаче регрессии — средним.\n",
    "\n",
    "Рекомендуется в задачах классификации брать $\\large m = \\sqrt{n}$, а в задачах регрессии — $\\large m = \\frac{n}{3}$, где $\\large n$ — число признаков. Также рекомендуется в задачах классификации строить каждое дерево до тех пор, пока в каждом листе не окажется по одному объекту, а в задачах регрессии — пока в каждом листе не окажется по пять объектов.\n",
    "\n",
    "Таким образом, случайный лес — это бэггинг над решающими деревьями, при обучении которых для каждого разбиения признаки выбираются из некоторого случайного подмножества признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Параметры\n",
    "\n",
    "\n",
    "Метод случайного леса реализован в библиотеке машинного обучения [scikit-learn](http://scikit-learn.org/stable/) двумя классами `RandomForestClassifier` и `RandomForestRegressor`.\n",
    "\n",
    "Полный список параметров случайного леса для задачи регрессии:\n",
    "\n",
    "**n_estimators** — число деревьев в \"лесу\" (по дефолту – 10)\n",
    "\n",
    "**criterion** — функция, которая измеряет качество разбиения ветки дерева (по дефолту — \"mse\" , так же можно выбрать \"mae\")\n",
    "\n",
    "**max_features** — число признаков, по которым ищется разбиение. Вы можете указать конкретное число или процент признаков, либо выбрать из доступных значений: \"auto\" (все признаки), \"sqrt\", \"log2\". По дефолту стоит \"auto\".\n",
    "\n",
    "**max_depth** — максимальная глубина дерева  (по дефолту глубина не ограничена)\n",
    "\n",
    "**min_samples_split** — минимальное количество объектов, необходимое для разделения внутреннего узла. Можно задать числом или процентом от общего числа объектов (по дефолту — 2)\n",
    "\n",
    "**min_samples_leaf** — минимальное число объектов в листе. Можно задать числом или процентом от общего числа объектов (по дефолту — 1)\n",
    "\n",
    "**min_weight_fraction_leaf** — минимальная взвешенная доля от общей суммы весов (всех входных объектов) должна быть в листе (по дефолту имеют одинаковый вес)\n",
    "\n",
    "**max_leaf_nodes** — максимальное количество листьев (по дефолту нет ограничения)\n",
    "\n",
    "**min_impurity_split** — порог для остановки наращивания дерева (по дефолту 1е-7)\n",
    "\n",
    "**bootstrap** — применять ли бустрэп для построения дерева (по дефолту True)\n",
    "\n",
    "**oob_score** — использовать ли out-of-bag объекты для оценки R^2 (по дефолту False)\n",
    "\n",
    "**n_jobs** — количество ядер для построения модели и предсказаний (по дефолту 1, если поставить -1, то будут использоваться все ядра)\n",
    "\n",
    "**random_state** — начальное значение для генерации случайных чисел (по дефолту его нет, если хотите воспроизводимые результаты, то нужно указать любое число типа int\n",
    "\n",
    "**verbose** — вывод логов по построению деревьев (по дефолту 0)\n",
    "\n",
    "**warm_start** — использует уже натренированую модель и добавляет деревьев в ансамбль (по дефолту False)\n",
    "\n",
    "Для задачи классификации все почти то же самое, мы приведем только те параметры, которыми `RandomForestClassifier` отличается от `RandomForestRegressor`\n",
    "\n",
    "**criterion** — поскольку у нас теперь задача классификации, то по дефолту выбран критерий \"gini\" (можно выбрать \"entropy\")\n",
    "\n",
    "**class_weight** — вес каждого класса (по дефолту все веса равны 1, но можно передать словарь с весами, либо явно указать \"balanced\", тогда веса классов будут равны их исходным частям в генеральной совокупности; также можно указать \"balanced_subsample\", тогда веса на каждой подвыборке будут меняться в зависимости от распределения классов на этой подвыборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры, на которые в первую очередь стоит обратить внимание при построении модели:\n",
    "- n_estimators — число деревьев в \"лесу\"\n",
    "- criterion — критерий для разбиения выборки в вершине\n",
    "- max_features — число признаков, по которым ищется разбиение\n",
    "- min_samples_leaf — минимальное число объектов в листе\n",
    "- max_depth — максимальная глубина дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Важность признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень часто вы хотите понять свой алгоритм, почему он именно так, а не иначе дал определенный ответ. Или если не понять его полностью, то хотя бы какие переменные больше всего влияют на результат. Из случайного леса можно довольно просто получить данную информацию.\n",
    "\n",
    "**Суть метода**\n",
    "\n",
    "По данной картинке интуитивно понятно, что важность признака «Возраст»  в задаче кредитного скоринга выше, чем важность признака «Доход» . Формализуется это с помощью понятия прироста информации.\n",
    "<img src=\"https://github.com/iakubovskii7/mlcourse.ai/blob/master/img/credit_scoring_toy_tree.gif?raw=true\" align='center'>\n",
    "\n",
    "Если построить много деревьев решений (случайный лес), то чем выше в среднем признак в дереве решений, тем он важнее в данной задаче классификации/регрессии. При каждом разбиении в каждом дереве улучшение критерия разделения (в нашем случае коэффициент Джини) — это показатель важности, связанный с переменной разделения, и накапливается он по всем деревьям леса отдельно для каждой переменной.\n",
    "\n",
    "Давайте немного углубимся в детали. Среднее снижение точности, вызываемое переменной, определяется во время фазы вычисления out-of-bag ошибки. Чем больше уменьшается точность предсказаний из-за исключения (или перестановки) одной переменной, тем важнее эта переменная, и поэтому переменные с бо́льшим средним уменьшением точности более важны для классификации данных. Среднее уменьшение коэффициента Джини (или ошибки mse в задачах регрессии) является мерой того, как каждая переменная способствует однородности узлов и листьев в окончательной модели случайного леса. Каждый раз, когда отдельная переменная используется для разбиения узла, коэффициент Джини для дочерних узлов рассчитывается и сравнивается с коэффициентом исходного узла. Коэффициент Джини является мерой однородности от 0 (однородной) до 1 (гетерогенной). Изменения в значении критерия разделения суммируются для каждой переменной и нормируются в конце вычисления. Переменные, которые приводят к узлам с более высокой чистотой, имеют более высокое снижение коэффициента Джини.\n",
    "\n",
    "А теперь представим все вышеописанное в виде формул. \n",
    "$$ \\large VI^{T} = \\frac{\\sum_{i \\in \\mathfrak{B}^T}I \\Big(y_i=\\hat{y}_i^{T}\\Big)}{\\Big |\\mathfrak{B}^T\\Big |} - \\frac{\\sum_{i \\in \\mathfrak{B}^T}I \\Big(y_i=\\hat{y}_{i,\\pi_j}^{T}\\Big)}{\\Big |\\mathfrak{B}^T\\Big |} $$\n",
    "\n",
    "$ \\large \\hat{y}_i^{(T)} = f^{T}(x_i)  $ — предсказание класса перед перестановкой/удалением признака\n",
    "$ \\large \\hat{y}_{i,\\pi_j}^{(T)} = f^{T}(x_{i,\\pi_j})   $ — предсказание класса после перестановки/удаления признака\n",
    "$ \\large x_{i,\\pi_j} = (x_{i,1}, \\dots , x_{i,j-1}, \\quad x_{\\pi_j(i),j}, \\quad x_{i,j+1}, \\dots , x_{i,p})$\n",
    "Заметим, что $ \\large VI^{(T)}(x_j) = 0 $, если $ \\large X_j $  не находится в дереве $ \\large T $ \n",
    "\n",
    "Расчет важности признаков в ансамбле:\n",
    "— ненормированные \n",
    "$$ \\large VI(x_j) = \\frac{\\sum_{T=1}^{N}VI^{T}(x_j)}{N} $$\n",
    "\n",
    "— нормированные \n",
    "$$ \\large z_j = \\frac{VI(x_j)}{\\frac{\\hat{\\sigma}}{\\sqrt{N}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преимущества и недостатки случайного леса\n",
    "\n",
    "**Плюсы**:\n",
    " - имеет высокую точность предсказания, на большинстве задач будет лучше линейных алгоритмов; точность сравнима с точностью бустинга\n",
    " - практически не чувствителен к выбросам в данных из-за случайного сэмлирования\n",
    " - нечувствителен к масштабированию (и вообще к любым монотонным преобразованиям) значений признаков, связано с выбором случайных подпространств\n",
    " - не требует тщательной настройки параметров, хорошо работает «из коробки». С помощью «тюнинга» параметров можно достичь прироста от 0.5 до 3% точности в зависимости от задачи и данных\n",
    " - способен эффективно обрабатывать данные с большим числом признаков и классов\n",
    " - одинаково хорошо обрабатывет как непрерывные, так и дискретные признаки\n",
    " - редко переобучается, на практике добавление деревьев почти всегда только улучшает композицию, но на валидации, после достижения определенного количества деревьев, кривая обучения выходит на асимптоту\n",
    " - для случайного леса существуют методы оценивания значимости отдельных признаков в модели\n",
    " - хорошо работает с пропущенными данными; сохраняет хорошую точность, если большая часть данных пропущенна\n",
    " - предполагает возможность сбалансировать вес каждого класса на всей выборке, либо на подвыборке каждого дерева\n",
    " - вычисляет близость между парами объектов, которые могут использоваться при кластеризации, обнаружении выбросов или (путем масштабирования) дают интересные представления данных\n",
    " - возможности, описанные выше, могут быть расширены до немаркированных данных, что приводит к возможноти делать кластеризацию и визуализацию данных, обнаруживать выбросы\n",
    " - высокая параллелизуемость и масштабируемость.\n",
    " \n",
    "**Минусы**:\n",
    " - в отличие от одного дерева, результаты случайного леса сложнее интерпретировать\n",
    " - нет формальных выводов (p-values), доступных для оценки важности переменных\n",
    " - алгоритм работает хуже многих линейных методов, когда в выборке очень много разреженных признаков (тексты, Bag of words)\n",
    " - случайный лес не умеет экстраполировать, в отличие от той же линейной регрессии (но это можно считать и плюсом, так как не будет экстремальных значений в случае попадания выброса)\n",
    " - алгоритм склонен к переобучению на некоторых задачах, особенно на зашумленных данных\n",
    " - для данных, включающих категориальные переменные с различным количеством уровней, случайные леса предвзяты в пользу признаков с большим количеством уровней: когда у признака много уровней, дерево будет сильнее подстраиваться именно под эти признаки, так как на них можно получить более высокую точность\n",
    " - если данные содержат группы коррелированных признаков, имеющих схожую значимость для меток, то предпочтение отдается небольшим группам перед большими\n",
    " - больший размер получающихся моделей. Требуется $O(NK)$ памяти для хранения модели, где $K$ — число деревьев.\n",
    "\n",
    "**Полезные источники:**\n",
    "- 15 раздел книги “[Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)” Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie\n",
    "- [Блог](https://alexanderdyakonov.wordpress.com/2016/11/14/случайный-лес-random-forest/) Александра Дьяконова\n",
    "- больше про практические применение случайного леса и других алгоритмов композиций в официальной документации [scikit-learn](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [Курс](https://github.com/esokolov/ml-course-hse) Евгения Соколова по машинному обучению (материалы на GitHub). Есть дополнительные практические задания для углубления ваших знаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бустинг\n",
    "\n",
    "Бустинг представляет собой жадный алгоритм построения композиции алгоритмов. Основная идея заключается в том, чтобы, имея множество относительно слабых алгоритмов обучения, построить их хорошую линейную комбинацию. Он похож на бэггинг тем, что базовый алгоритм обучения фиксирован. Отличие состоит в том, что обучение базовых алгоритмов для композиции происходит итеративно, и каждый следующий алгоритм стремится компенсировать недостатки композиции всех предыдущих алгоритмов.\n",
    "\n",
    "На примере бустинга стало ясно, что хорошим качеством могут обладать сколь угодно сложные композиции классификаторов, при условии, что они правильно настраиваются. Это развеяло существовавшие долгое время представления о том, что для повышения обобщающей способности необходимо ограничивать сложность алгоритмов. \n",
    "\n",
    "Впоследствии этот феномен бустинга получил теоретическое обоснование. Оказалось, что взвешенное голосование не увеличивает эффективную сложность алгоритма, а лишь сглаживает ответы базовых алгоритмов. Эффективность бустинга объясняется тем, что по мере добавления базовых алгоритмов увеличиваются отступы обучающих объектов. Причём бустинг продолжает раздвигать классы даже после достижения безошибочной классификации обучающей выборки.\n",
    "\n",
    "**Общая схема бустинга**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг\n",
    "Метод градиентного бустинга в некотором смысле является обобщением остальных методов бустинга, поскольку он позволяет оптимизировать произвольную дифференцируемую функцию потерь. Данный алгоритм похож на метод градиентного спуска, применяемый для решения задач оптимизации. Основная идея заключается в том, что каждый следующий добавляемый в композицию алгоритм настраивается на остатки предыдущих алгоритмов.\n",
    "\n",
    "Задача градиентного бустинга - обучение $N$-ой модели:\n",
    "\n",
    "$$ \\Large \\frac{1} {\\ell} \\sum_{i=1}^{\\ell} L (y_i, a_{N-1}(x_i) + b_N(x_i)) \\rightarrow  \\underset{b_N(x)}{min}$$\n",
    "\n",
    "$\\Large a_{N-1}(x_i)$ - прогноз алгоритма на $N-1$ шаге\n",
    "\n",
    "Как посчитать и куда сдвигать $\\large a_{N-1}(x_i)$, чтобы уменьшить ошибку? Взять производную!\n",
    "\n",
    "$$ \\Large s_{i}^{N} = -\\frac{\\partial}{\\partial z} L(y_i, z) \\ в \\ точке \\ \\large z = a_{N-1}(x_i) $$\n",
    "\n",
    "Знак этой производной будет показывать, в какую сторону сдвигать прогноз на $x_i$, чтобы уменьшить ошибку композиции на нем. Величина показывает, насколько сильно мы можем уменьшить ошибку при сдвиге предсказания. Если сдвиг $s_{i}^{N} > 0$, то прогноз увеличиваем, в противном случае - уменьшаем. \n",
    "\n",
    "Теперь наша задача немного меняется:\n",
    "\n",
    "$$ \\Large \\frac{1} {\\ell} \\sum_{i=1}^{\\ell} (b_N(x_i) - s_{i}^{N})^2 \\rightarrow  \\underset{b_N(x)}{min}$$\n",
    "\n",
    "Пусть $\\large L(y_i, z) = MSE$. Тогда $\\large s_i^N = y_i - a_{N-1}(x_i)$. В итоге минимизируем следующую функцию потерь (`задача регрессии`):\n",
    "\n",
    "$$ \\Large \\frac{1} {\\ell} \\sum_{i=1}^{\\ell} [b_N(x_i) - (y_i - a_{N-1}(x_i))]^2 \\rightarrow  \\underset{b_N(x)}{min}$$\n",
    "\n",
    "Пусть $\\large L(y_i, z) = logloss$. Тогда минимизируем следующую функцию потерь (`задача классификации`):\n",
    "\n",
    "$$ \\Large \\frac{1} {\\ell} \\sum_{i=1}^{\\ell} [b_N(x_i) - \\frac{y_i}{1 - exp(y_i a_{N-1}(x_i))} ]^2 \\rightarrow  \\underset{b_N(x)}{min}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что при бэггинге смещение не изменяется, а разброс снижается. В случае градиентного бустинга смещение базовых моделей снижается, но может возрасти дисперсия. В этом случае в качестве базовых моделей нужно брать `неглубокие` деревья. \n",
    "\n",
    "Еще одна проблема бустинга - из-за простоты базовых моделей. мы можем не найти то направление, куда нужно идти, чтобы снизить ошибку (например, базовая модель говорит нам об одном направлении движения, а в реальности нужно идти в другую сторону). В этом случае мы можем добавлять базовые модели с некоторым весом: $\\large a_N(x) = a_{N-1}(x_i) + \\eta b_N(x_i)$, $\\eta \\in (0;1]$ - длина шага. Здесь $\\large \\eta$ - регуляризация композиции. За счет этого вклад отдельных моделей в композицию снижается. Как правило, чем меньше $\\eta$, тем больше деревьев. \n",
    "\n",
    "Еще один способ снизить переобучение моделей бустинга - обучать деревья на случайных подмножествах признаков (как и в случайном лесе). Помимо этого, можно еще случайно выкидывать часть наблюдений из выборки.\n",
    "\n",
    "Ниже рассмотрим основные подходы, которые используются в градиентном бустинге: \n",
    "\n",
    "- ODT - oblivious decision trees\n",
    "\n",
    "- Leaf-wise (best-first) tree growth\n",
    "\n",
    "- Level-wise (depth-first) tree growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODT - oblivious decision trees\n",
    "\n",
    "На уровне одного дерева мы используем один и тот же признак, потому что он ускоряет алгоритмы в несколько раз и в то же время не ухудшает результаты. Это происходит из-за того, что композиция `простых` моделей дает лучшие результаты. Количество одинаковых операций равно $n-1$, где $n$ - глубина дерева. Такие деревья сбалансированы, менее склонны к переобучению и позволяют значительно ускорить предсказание во время тестирования.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*AjrRnwvBuu-zK8CvEfM29w.png)\n",
    "\n",
    "Одним из самых популярных примеров является [CatBoost](https://catboost.ai/), созданный [Яндексом](https://en.wikipedia.org/wiki/Yandex). \n",
    "\n",
    "CatBoost - это высокопроизводительная библиотека с открытым исходным кодом для градиентного бустинга на деревьях решений.\n",
    "\n",
    "CatBoost используется для поиска, рекомендательных систем, персональных помощников, самодвижущихся автомобилей, предсказания погоды и многих других задач в Яндексе и других компаниях, включая CERN, Cloudflare, такси Careem. Он имеет открытый исходный код и может быть использован любым желающим.\n",
    "\n",
    "**Категориальные особенности** в CatBoost\n",
    "\n",
    "CatBoost использует комбинации категориальных признаков в качестве дополнительных категориальных признаков, которые захватывают зависимости высокого порядка, такие как совместная информация ID пользователя и темы объявления в задаче предсказания рекламных кликов. Количество возможных комбинаций растет экспоненциально с ростом числа категориальных признаков в наборе данных, и обработать их все не представляется возможным. CatBoost строит комбинации жадным способом. А именно, для каждого разбиения дерева CatBoost объединяет (конкатенирует) все категориальные признаки (и их комбинации), уже использованные для предыдущих разбиений текущего дерева, со всеми категориальными признаками в наборе данных. Комбинации преобразуются в TS на лету.\n",
    "\n",
    "CatBoost реализует алгоритм, который позволяет бороться с обычными предубеждениями градиентного бустинга. Существующие реализации сталкиваются со статистической проблемой - смещением предсказания. Эта проблема похожа на ту, что возникает при предварительной обработке категориальных переменных, которая была описана выше.\n",
    "\n",
    "Команда Catboost разработала упорядоченный бустинг, модификацию стандартного алгоритма градиентного бустинга, которая позволяет избежать смещения (но это не точно).\n",
    "\n",
    "Github этой библиотеки размещен [здесь](https://github.com/catboost).\n",
    "\n",
    "[Установка](https://catboost.ai/docs/concepts/python-installation.html)\n",
    "\n",
    "Основные **преимущества** CatBoost:\n",
    "\n",
    "- отличное качество без подстройки параметров\n",
    "\n",
    "- поддержка категориальных признаков (вам не нужна предварительная обработка категориальных признаков)\n",
    "\n",
    "- быстрая и масштабируемая GPU-версия\n",
    "\n",
    "- быстрое предсказание и повышенная точность\n",
    "\n",
    "- вы можете написать собственную функцию потерь\n",
    "\n",
    "- предоставляет инструменты для пакета Python, которые позволяют строить графики с различной статистикой обучения. Доступ к этой информации возможен как во время, так и после процедуры обучения\n",
    "\n",
    "- его можно интегрировать в Tensorflow. Например, это распространенный случай объединения Catboost и Tensorflow. Нейронная сеть может быть использована для извлечения признаков для градиентного бустинга.\n",
    "\n",
    "Основные **недостатки** CatBoost:\n",
    "\n",
    "- несмотря на хорошее прогнозирование, как утверждает Яндекс, качество прогнозов во многих реальных задачах выглядит хуже, чем в случае других алгоритмов (см., например, [здесь](https://www.kdnuggets.com/2018/03/catboost-vs-light-gbm-vs-xgboost.html)).\n",
    "\n",
    "- ...придумайте что-то еще\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf-wise (best-first) tree growth\n",
    "\n",
    "Данный метод подразумевает построение дерева до ***максимального количества листьев***. Среди всех вариантов листьев выбираем тот вариант, расщепление при котором сильно уменьшает ошибку.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*P1uHwsMu_f0zGEvh4YK0kg.png)\n",
    "\n",
    "Популярный пример такого вида бустинга - алгоритм [LightGBM](https://lightgbm.readthedocs.io/en/latest/Features.html). В 2017 году компания microsoft создала LightGBM как альтернативу использованию XGBoost. LightGBM может быть использован в Python, R и C++. \n",
    "\n",
    "Помимо `leaf-wise` подхода, основное его отличие от других видов бустинга заключается в поиске порогов для разбиения деревьев на узлы при помощи производных.\n",
    "\n",
    "Как указано в документации, LightGBM является улучшением алгоритма градиентного бустинга с точки зрения *эффективности*, *скорости*, поддержки распределенной *параллельной обработки* и *GPU*.\n",
    "LightGBM подходит для использования, если вы хотите построить модель с большим количеством данных. Если у вас всего 100 точек, лучше использовать другой алгоритм машинного обучения, так как в этом случае возможно переобучение.\n",
    "В [документации  LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) обращают внимание на следующие важные параметры:\n",
    "\n",
    "- max_depth: максимальная глубина дерева\n",
    "\n",
    "- num_leaves: количество листьев в дереве; рекомендуется настривать его меньше, чем 2^(max_depth) (например, 2^(max_depth)/1.5)\n",
    "\n",
    "- min_data_in_leaf: минимальное количество наблюдений, которое может иметь лист дерева - маленькое значение приводит к\n",
    "слишком глубокому и переобученному дереву. Рекомендуется устанавливать на уровне 3,4 - значных числе\n",
    "\n",
    "- learning_rate: размер шага для каждой итерации при движении к минимуму функции потерь в градиентном спуске.\n",
    "\n",
    "- feature_fraction: доли признаков/параметров, которые будут случайным образом выбираться на каждой итерации для построения деревьев\n",
    "\n",
    "- bagging_fraction: задает долю от выборки, которая будут использоваться в каждой итерации для создания нового набора данных\n",
    "\n",
    "- min_split_gain: минимальное значение прироста информации при разделения дерева. Увеличение данного параметра может\n",
    "ускорить обучение, так как не любой прирост информации может быть важен для нас.\n",
    "\n",
    "- num_iterations: количество базовых алгоритмов. Измененять лучше одновременно с *learning_rate*\n",
    "\n",
    "- reg_lambda: коэффициент L1 регуляризации для решения проблемы переобучения и выбора признаков\n",
    "\n",
    "- reg_alpha: коэффициент L2 регуляризации для решения проблемы переобучения и выбора признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level-wise (depth-first) tree growth\n",
    "\n",
    "В данном подходе дерево строится рекурсивно до тех пор, пока не будет достигнута ***максимальная глубина***.\n",
    "\n",
    "![](https://i.stack.imgur.com/e1FWe.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost\n",
    "\n",
    "Самым популярным алгоритмом бустинга, как уже говорилось, является [XGboost](https://xgboost.readthedocs.io/en/latest/).\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*l4PN8hyAO4fMLxUbIxcETA.png)\n",
    "\n",
    "\n",
    "[Xgboost](https://github.com/dmlc/xgboost) использует еще больше параметров для регуляризации базовых деревьев.\n",
    "\n",
    "Целевая функция для оптимизации в Xgboost состоит из двух слагаемых: специфичной пункции потерь и регуляризатора $\\Omega (f_k)$ для каждого из $K$ деревьев, где $f_k$ - прогноз $k$-ого дерева.\n",
    "\n",
    "\n",
    "$$\n",
    "obj(\\theta) = \\sum_{i}^{\\ell} l(y_i - \\hat{y_i}) +  \\sum_{k=1}^{K} \\Omega (f_k)\n",
    "$$\n",
    "\n",
    "Функция потерь зависит от решаемой задачи (Xgboost адаптирован под задачи классификации, регрессии и ранжирования, подробней хорошо описано в [документации](http://xgboost.readthedocs.io/) Xgboost), а регуляризатор выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2\n",
    "$$\n",
    "\n",
    "Первое слагаемое ($\\gamma T$) штрафует модель за большое число листьев $T$, а второе ($\\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2$) контролирует сумму весов модели в листьях. \n",
    "\n",
    "Добавим также, что сама процедура подбора деревьев устроена так, что в функции, при помощи которой подбираются признаки для разбиения, уже встроены первая и вторая производная исходной функции потерь. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные различия между CatBoost, LightGBM, XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расщепление узлов \n",
    "\n",
    "![](https://miro.medium.com/max/700/1*E006sjlIjabDJ3jNixRSnA.png)\n",
    "\n",
    "Перед обучением все алгоритмы создают пары признак-сплит для всех факторов. Например, (возраст, <5), (возраст, >10), (сумма, >500). Эти пары признак-сплит строятся на основе гистограммы и используются во время обучения в качестве возможных расщеплений узлов. Этот метод предварительной обработки быстрее, чем точный жадный алгоритм, который линейно перечисляет все возможные разбиения для непрерывных признаков (как в классических деревьях решений).\n",
    "\n",
    "**LightGBM** предлагает градиентное одностороннее сэмплирование (*GOSS*), которое выбирает разбиения, используя все наблюдения с большими значениями градиентов (т.е. с большой ошибкой) и случайную выборку наблюдений с малыми значениями градиентов. Чтобы сохранить одинаковое распределение данных при вычислении информационного прироста, *GOSS* вводит постоянный множитель для наблюдений данных с малыми градиентами. Таким образом, GOSS достигает хорошего баланса между увеличением скорости за счет уменьшения количества наблюдений и сохранением точности для обученных деревьев решений. Этот метод не является методом по умолчанию для LightGBM, поэтому его следует выбирать явно.\n",
    "\n",
    "**Catboost** предлагает новую технику под названием Minimal Variance Sampling (*MVS*), которая представляет собой взвешенное сэмплирование Stochastic Gradient Boosting. В этой технике взвешенное сэмплирование происходит на уровне дерева, а не на уровне разбиения. Наблюдения для каждого дерева отбираются таким образом, чтобы максимизировать точность оценки разбиения.\n",
    "\n",
    "**XGboost** не использует никаких методов взвешенной выборки, что делает его процесс разбиения более медленным по сравнению с *GOSS* и MVS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как растут листья из узлов\n",
    "\n",
    "**Catboost** строит сбалансированное дерево. На каждом уровне такого дерева выбирается пара признак-сплит, которая приносит наименьшие потери (согласно функции потерь) и используется для всех узлов данного уровня. Можно изменить с помощью параметра *grow-policy*.\n",
    "\n",
    "**LightGBM** использует рост дерева по листьям (по принципу \"лучший-первый\"). Выбирается тот лист, который минимизирует функцию потерь, допуская рост несбалансированного дерева. Поскольку дерево растет не по уровням, а по листьям, при небольшой выборке может произойти переобучение. В таких случаях важно контролировать глубину дерева.\n",
    "\n",
    "**XGboost** расщепляет узлы до заданного гиперпараметра *max_depth*, а затем начинает обрезать дерево в обратном направлении и удаляет части, за пределами которых нет положительного прироста информации. Это работает, поскольку иногда за расщеплением без снижения функции потерь может следовать расщепление с уменьшением функции потерь. XGBoost также может выполнять рост дерева по листьям (как LightGBM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Важность признаков\n",
    "\n",
    "**Catboost** имеет два способа вычисления важности признаков. \n",
    "\n",
    "Первый - *PredictionValuesChange*. Для каждого признака *PredictionValuesChange* показывает, насколько в среднем изменяется предсказание при изменении значения признака. Признак будет иметь большую важность, если изменение значения признака вызывает большое изменение в прогнозируемом значении. Это метод расчета важности признака по умолчанию для неранжированных метрик. \n",
    "\n",
    "Второй метод - *LossFunctionChange*. Этот тип важности признака может использоваться для любой модели, но особенно полезен для моделей ранжирования. Для каждого признака значение представляет собой разницу между значением функции потерь модели с этим признаком и без него. Поскольку переоценка модели без одного из признаков требует больших вычислительных затрат, эта модель строится приблизительно, используя исходную модель с этим признаком, удаленным из всех деревьев в ансамбле. \n",
    "\n",
    "**LightGBM** и **XGBoost** имеют два похожих метода. \n",
    "\n",
    "Первый - *Gain*, который представляет собой улучшение точности (или общий выигрыш), приносимое признаком узлам, на которых он находится. \n",
    "\n",
    "Второй метод имеет разное название в каждом пакете: *split* (LightGBM) и *Frequency/Weight* (XGBoost). Этот метод вычисляет относительное количество раз, когда определенный признак встречается во всех расщеплениях деревьев модели. Этот метод может быть необъективным для категориальных признаков с большим количеством категорий.\n",
    "\n",
    "У **XGBoost** есть еще один метод, *reach*, который представляет собой относительное количество наблюдений, связанных с признаком. Для каждого признака мы подсчитываем количество наблюдений, используемых для выбора узла листа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ансамбли: основные выводы\n",
    "\n",
    "1. Случайный лес - деревья строятся параллельно на бутстрапированных подвыборках и полностью с ограничением на максимальную глубину. Результат агрегируется. Деревья некоррелируемые друг с другом. Число признаков для разбиения выбирается, как правило, равное квадратному корню из размерности признакового пространства. Проблема - вычислительно затратно.\n",
    "\n",
    "2. Бустинг - деревья строятся последовательно, на каждой итерации снижая ошибку, которая получается из композиции предыдущих базовых алгоритмов. \n",
    "\n",
    "3. CatBoost хорошо работает с категориальными признаками. На каждом разбиении использует один признак. Относительно быстрый.\n",
    "\n",
    "4. LightGBM не ограничивает глубину, но ограничивает количество листьев. Самый быстрый алгоритм бустинга на всем диком мире мэшин лернинга.\n",
    "\n",
    "5. XGBoost - чемпион по увеличению скора на 0.0000001 сотых.\n",
    "\n",
    " - **Catboost** - однаковые признаки для каждого уровня разбиения\n",
    "\n",
    " - **LightGBM** - ставим ограничение на количество листьев \n",
    "\n",
    " - **XGboost** - ограничение на максимальную глубину дерева и обрезка в обратном направлении\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}